{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<head>\n",
    "    <h1>Semana 9 Exercicios 1 e 2<h1>\n",
    "<head>\n",
    "    <div>\n",
    "    <img align=\"left\" alt=\"Geron-Mattoso\" height=\"100\" width=\"150\" \n",
    "src=\"https://spark.apache.org/images/spark-logo-rev.svg\">\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 16:18:04 WARN Utils: Your hostname, DESKTOP-QF8356S resolves to a loopback address: 127.0.1.1; using 192.168.50.201 instead (on interface eth0)\n",
      "22/11/21 16:18:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 16:18:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geron/.local/share/virtualenvs/Compass-_AkEn2Ee/lib/python3.8/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import pyspark.sql.functions as fc\n",
    "import pyspark.sql.types\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "        .appName(\"Spark Exemplos\")\\\n",
    "            .getOrCreate()\n",
    "\n",
    "sc = SQLContext(spark.sparkContext)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1 - Crie um array de 250, Int e aplique o método reverse para inverter o conteúdo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 01|\n",
      "| 11|\n",
      "| 21|\n",
      "| 31|\n",
      "| 41|\n",
      "+---+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arraySpark = spark.range(250)\n",
    "\n",
    "arraySpark.select(fc.reverse(arraySpark.id).alias('id')).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2 - Crie uma lista de 20 animais, ordene-os e itere para imprimi-las individualmente cada um deles. Depois salve num arquivo texto em formato CS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|animais|\n",
      "+-------+\n",
      "| cavalo|\n",
      "|   vaca|\n",
      "|   rato|\n",
      "| girafa|\n",
      "|   anta|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+\n",
      "|animais|\n",
      "+-------+\n",
      "|   anta|\n",
      "|babuino|\n",
      "|    boi|\n",
      "|carcaju|\n",
      "| cavalo|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dados = [[\"cavalo\"], [\"vaca\"],[\"rato\"], [\"girafa\"], [\"anta\"], [\"quero-quero\"],[\"tico-tico\"],[\"lambari\"], [\"jundia\"], [\"pato\"], \\\n",
    "             [\"gato\"], [\"galo\"], [\"iguana\"], [\"pinguin\"], [\"boi\"], [\"macaco\"], [\"zebra\"], [\"babuino\"], [\"carcaju\"], [\"foca\"]]\n",
    "\n",
    "\n",
    "schema = [(\"animais\")]\n",
    "\n",
    "array = spark.createDataFrame(data = dados, schema = schema)\n",
    "\n",
    "array.show(5)\n",
    "\n",
    "array.select('animais').orderBy('animais').show(5)\n",
    "\n",
    "array.write.csv('./output', mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4 (tags/v3.9.4:1f2e308, Apr  6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2deb7cf860cc7fd3a77031a4d28667aa31f29c46dfc55481c8bdaeda7d90f5bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
